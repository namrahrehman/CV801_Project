{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKsvhfW_X7cf"
   },
   "outputs": [],
   "source": [
    "# Point to the folder that contains dataset_dict.json (NOT directly to train/)\n",
    "VQARAD_PATH = \"/content/drive/MyDrive/data/VQA_RAD\"\n",
    "\n",
    "# Do the same for SLAKE when you place it (adjust this to your actual path):\n",
    "SLAKE_PATH  = \"/content/drive/MyDrive/data/SLAKE\"\n",
    "\n",
    "OUT_DIR = \"/content/drive/MyDrive/data/outputs_cfproxy\"\n",
    "!mkdir -p \"$OUT_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the SLAKE dataset (points to the folder containing dataset_dict.json)\n",
    "slake = load_from_disk(SLAKE_PATH)   # returns a DatasetDict because dataset_dict.json exists\n",
    "print(slake)                         # Expect: DatasetDict with 'train' and 'test'\n",
    "\n",
    "# Pick a split\n",
    "slake_train = slake[\"train\"]\n",
    "slake_test  = slake[\"test\"]\n",
    "\n",
    "# Quick sanity check\n",
    "row = slake_test[0]\n",
    "print(row.keys())       # -> dict_keys(['image','question','answer', ...])\n",
    "print(row[\"question\"], \"->\", row[\"answer\"])\n",
    "row[\"image\"].show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_x1qdG82YwCb",
    "outputId": "3da73dcb-6274-4bae-f869-ca82bb2881db"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'question', 'answer'],\n",
      "        num_rows: 1793\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'question', 'answer'],\n",
      "        num_rows: 451\n",
      "    })\n",
      "})\n",
      "dict_keys(['image', 'question', 'answer'])\n",
      "is there evidence of an aortic aneurysm? -> yes\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip -q install \"transformers>=4.44\" accelerate torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "!pip -q install datasets pillow opencv-python rapidfuzz tqdm matplotlib"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2HjWTjtZBkM",
    "outputId": "dbc03c99-174c-480e-b28e-563d8de29cb8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.2/3.2 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[90m\u257a\u001b[0m\u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.5/3.2 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "SLAKE_PATH = \"/content/drive/MyDrive/data/SLAKE\"\n",
    "OUT_DIR = \"/content/drive/MyDrive/data/outputs_cfproxy\"\n",
    "\n",
    "from pathlib import Path\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from datasets import load_from_disk\n",
    "slake = load_from_disk(SLAKE_PATH)\n",
    "ds_train, ds_test = slake[\"train\"], slake[\"test\"]\n",
    "\n",
    "def iter_samples(ds, n=None):\n",
    "    m = len(ds) if n is None else min(n, len(ds))\n",
    "    for i in range(m):\n",
    "        r = ds[i]\n",
    "        yield i, r[\"image\"], r[\"question\"], r[\"answer\"]"
   ],
   "metadata": {
    "id": "GecUeCabZOLq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if DEVICE==\"cuda\" else torch.float32,\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265,
     "referenced_widgets": [
      "2fc1ba01333040caa746220cae219bf7",
      "7fd7b7c1e38943b897ae4df08c138648",
      "1ecd1002bcfb4f8a8286537310ae9702",
      "562798418b90402a8c388a3831651dc9",
      "535e01633a834e18b0f2de24ca1554f6",
      "d3c6721b8caa4221b02223311cac9a67",
      "314a78b6db5640d0a15d5cc5669b4dcc",
      "4ddff2b15190473b8add04247b5c64f1",
      "6f9fd195caf44d4cab1283c8b0605a03",
      "4a9622ae8cb442ce85cc338edc6c8ea3",
      "b528efc4d1d3413692742938692ba6dc",
      "44d9f50de7034b63a3e05bc567e39e1e",
      "20f7f11f46b6401daa3c25c5a2b3ede6",
      "703fe3a20b5c48bcb9bb18234c18350e",
      "c0901fda35a74cf09056d494abf7b51e",
      "39a81edf4fd1476a947c1042738b82b6",
      "3d0e2134033f469099e49463714ddf54",
      "85fadbeab6a64fe7b81336aab802fea9",
      "77026f4d7b194f9e8c91de2afe7a4058",
      "1e56462925ad4b5c902e82fff04e328c",
      "d3edb4a57af04767bd9df6e25bfdbcd6",
      "78c13322d38b4904a5387a2a41e24b91",
      "053022b6a42c4b869be70ccbe4171d35",
      "bd42bd821c3a41ec8f5cd4dae3c6f6fc",
      "247414552b9c4c058c83c31e775ad8fb",
      "f5b54c541eb743e88a9dcee3c3d3fff6",
      "6349cd7ca1224f16b248c1bc22095d36",
      "cfc9d74e3a0742568d611907e9ca2e8b",
      "8b85ebb7b29f4fd5b57b2a7d01e5be3e",
      "af16b59930044a8daefc72219622d04f",
      "0f3b1777bcef4dc99d262aed49fa2f2d",
      "e4db958aa6214f6eb02ff32e3829779c",
      "2a184c1d25c647b3b73f7acee72c473a",
      "d6335641677743019099bd88ab1b9049",
      "f0de603fa1864cdca87f918685651353",
      "b5821893a06d4fb5a95f42e2d98c4a58",
      "f6736cdeb42d4ae6a2bc05c65ba4b285",
      "31311b2e373d437094db9e11750981e8",
      "6f2a44f967404e0d8aaf513136166c60",
      "058ecb1c36d9473cb683e07e75db743b",
      "60d383c0e13f4512a8c83b561f3c32e0",
      "89216b49a1a84926af4461d12745c67a",
      "4d48aece56aa45f596f5b1dfadc7d0f4",
      "0a7c03b1c78f40ecb14918ce9a58e57d",
      "a19f64bde5da4256ad9482af52ebcef7",
      "7eaa1af06a5c4c4cb663138ee1b783be",
      "64c1547f912e4678b0d9cd581d2c0f75",
      "8d0ae8aad558497590ccb61cd94e6e33",
      "a1dd5af723834c01806fef32f47c3665",
      "82044897a46e4611b2f1180f06e26d61",
      "8073878aae69433d9c61a25cb4a6fff7",
      "00e10d417d1f4d47a84194d3f09f77a7",
      "7b3a3bb8e28e4f5a81039e17777317c9",
      "37406c6fd5b84615a2834df30ffc8b48",
      "0020f5f57902448b846c7818f903a9b9",
      "f10fae6a34b746cd9af7a1374541be8d",
      "0c8eca398f14420a92e173c5884099ae",
      "6676c515aa604adc8474496fb18fd40d",
      "0972e9ff065d4b6d8a25632b7164bbed",
      "78f37f304a1f491cb10a8e92b0712abb",
      "47ead9c019054b50b249adf8daef6d0f",
      "0e14730d25d8453fa0bf544ea983ce1a",
      "2443b844c6f240b7b1d7eed4d9604b01",
      "f728bdd9c89c456a9e783afc38bf075d",
      "b50886697f8142eeb0b89f102a0351aa",
      "551572f57c0a4904a672859a4cc4bcac"
     ]
    },
    "id": "sdXy--sWZPiZ",
    "outputId": "1bebcc00-00d3-4461-c6f4-3ed4e2bc7a78"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fc1ba01333040caa746220cae219bf7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44d9f50de7034b63a3e05bc567e39e1e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "053022b6a42c4b869be70ccbe4171d35"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6335641677743019099bd88ab1b9049"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a19f64bde5da4256ad9482af52ebcef7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f10fae6a34b746cd9af7a1374541be8d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def prompt_think_answer(q):\n",
    "    return f'''You are a medical VQA assistant. Think carefully but return ONLY valid JSON:\n",
    "{{\"answer\":\"<short answer>\"}}\n",
    "\n",
    "Rules:\n",
    "- Do not use code fences or backticks.\n",
    "- No extra keys or text beyond the JSON.\n",
    "- If your output is not valid JSON, immediately try again and output ONLY valid JSON.\n",
    "Question: \"{q}\"'''\n",
    "\n",
    "def prompt_caption_reason_answer(q):\n",
    "    return f'''You are a medical VQA assistant. First DESCRIBE the image, then REASON, then ANSWER.\n",
    "Return ONLY valid JSON with exactly these keys:\n",
    "{{\n",
    " \"caption\":\"<1-2 precise sentences about visible anatomy/findings>\",\n",
    " \"reasoning\":[\"<step1>\",\"<step2>\",\"<step3>\"],\n",
    " \"boxes\":[[x1,y1,x2,y2]],\n",
    " \"answer\":\"<short answer>\"\n",
    "}}\n",
    "Rules:\n",
    "- Do not use code fences or backticks.\n",
    "- No extra keys or text beyond the JSON.\n",
    "- Use integers for box coordinates within image bounds.\n",
    "- If your output is not valid JSON, immediately try again and output ONLY valid JSON.\n",
    "- Output at most 1 box tightly enclosing the most diagnostic finding (avoid full-organ boxes).\n",
    "- The box must be as small as possible while still covering the key evidence.\n",
    "\n",
    "Question: \"{q}\"'''"
   ],
   "metadata": {
    "id": "X_yswTaZZUBa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json, re\n",
    "\n",
    "def parse_json_safe(text):\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return None\n",
    "    chunk = m.group(0)\n",
    "    # minimal repairs: strip trailing commas before } or ]\n",
    "    chunk = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", chunk)\n",
    "    try:\n",
    "        return json.loads(chunk)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def call_vlm(image_pil, prompt, max_new_tokens=384, temperature=0.2, top_p=0.9):\n",
    "    # Messages with system role + image attached to the user turn\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful medical VQA assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},                      # placeholder token in chat text\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 1) Get chat string (not tensors)\n",
    "    chat_str = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 2) Build tensors from (chat string + image)\n",
    "    inputs = processor(\n",
    "        text=[chat_str],\n",
    "        images=[image_pil],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]  # number of tokens in the prompt\n",
    "\n",
    "    # 3) Generate\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=(temperature > 0),\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=processor.tokenizer.eos_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # 4) Decode only the NEW tokens\n",
    "    new_tokens = output_ids[0, input_len:]\n",
    "    out = processor.tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    return out  # parse_json_safe(out) should now find the JSON"
   ],
   "metadata": {
    "id": "fY5TLu8VbIqs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "i, img, q, gold = next(iter(iter_samples(ds_test, n=1)))\n",
    "print(\"Q:\", q, \"| gold:\", gold)\n",
    "\n",
    "raw = call_vlm(img, prompt_caption_reason_answer(q))\n",
    "print(raw[:500])            # should now show JSON or at least model text, not the system/user prelude\n",
    "print(parse_json_safe(raw)) # should be a dict (or None if malformed)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzn-Lr7kbS5S",
    "outputId": "1b06c30c-b8c0-4b9b-a3fb-6ba9094c1e19"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q: is there evidence of an aortic aneurysm? | gold: yes\n",
      "```json\n",
      "{\n",
      "  \"caption\": \"The image shows a chest X-ray with a pacemaker and no evidence of an aortic aneurysm.\",\n",
      "  \"reasoning\": [\n",
      "    \"The pacemaker is located in the chest cavity, not in the aorta.\",\n",
      "    \"There are no visible signs of an aortic aneurysm such as bulging or enlargement of the aorta.\"\n",
      "  ],\n",
      "  \"boxes\": [\n",
      "    [100, 100, 850, 900]\n",
      "  ],\n",
      "  \"answer\": \"No, there is no evidence of an aortic aneurysm.\"\n",
      "}\n",
      "```\n",
      "{'caption': 'The image shows a chest X-ray with a pacemaker and no evidence of an aortic aneurysm.', 'reasoning': ['The pacemaker is located in the chest cavity, not in the aorta.', 'There are no visible signs of an aortic aneurysm such as bulging or enlargement of the aorta.'], 'boxes': [[100, 100, 850, 900]], 'answer': 'No, there is no evidence of an aortic aneurysm.'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# run_protocol(ds_test, f\"{OUT_DIR}/vqarad_think_20.jsonl\", prompt_think_answer, limit=20)\n",
    "# run_protocol(ds_test, f\"{OUT_DIR}/vqarad_cap_20.jsonl\",   prompt_caption_reason_answer, limit=20)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLXLMxJqdaQW",
    "outputId": "bdff4ebb-1cd1-42dd-e527-70bad0814731"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:23<00:00,  1.16s/it]\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [03:08<00:00,  9.45s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Inference for 200\n",
    "run_protocol(ds_test, f\"{OUT_DIR}/slake_think.jsonl\", prompt_think_answer, limit=200)\n",
    "run_protocol(ds_test, f\"{OUT_DIR}/slake_cap.jsonl\",   prompt_caption_reason_answer, limit=200)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VF3otl2XyWyF",
    "outputId": "7b9aac07-09d2-4596-e54c-1b4e99f371ad"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [04:00<00:00,  1.20s/it]\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [27:09<00:00,  8.15s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json, re\n",
    "from rapidfuzz.fuzz import partial_ratio\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _get(d, *keys, default=None):\n",
    "    for k in keys:\n",
    "        if k in d: return d[k]\n",
    "    return default\n",
    "\n",
    "def has_json(r):\n",
    "    return isinstance(r, dict) and isinstance(r.get(\"json\"), dict)\n",
    "\n",
    "def has_boxes(r):\n",
    "    if not has_json(r):\n",
    "        return False\n",
    "    b = r[\"json\"].get(\"boxes\", None)\n",
    "    return isinstance(b, list) and len(b) > 0\n",
    "\n",
    "YN = {\"y\":\"yes\",\"yes\":\"yes\",\"true\":\"yes\",\"1\":\"yes\",\n",
    "      \"n\":\"no\",\"no\":\"no\",\"false\":\"no\",\"0\":\"no\"}\n",
    "\n",
    "def normalize_answer(a: str) -> str:\n",
    "    if not a:\n",
    "        return \"\"\n",
    "    a = str(a).strip().lower()\n",
    "    a = re.sub(r\"\\s+\", \" \", a)\n",
    "    # unify yes/no style answers\n",
    "    if a in YN:\n",
    "        return YN[a]\n",
    "    if a.startswith(\"y\"):\n",
    "        return \"yes\"\n",
    "    if a.startswith(\"n\"):\n",
    "        return \"no\"\n",
    "    # remove non-alphanum except space, dot, dash\n",
    "    a = re.sub(r\"[^a-z0-9 .-]\", \"\", a)\n",
    "    # collapse multiple spaces/dots\n",
    "    a = re.sub(r\"\\s+\", \" \", a).strip()\n",
    "    return a\n",
    "\n",
    "def accuracy(records):\n",
    "    ok=n=0\n",
    "    for r in records:\n",
    "        if not has_json(r):\n",
    "            continue\n",
    "        pred = normalize_answer(_get(r[\"json\"], \"answer\", \"pred\", default=\"\"))\n",
    "        gold = normalize_answer(_get(r, \"gold\", \"label\", \"answer\", default=\"\"))\n",
    "        if pred and gold:\n",
    "            ok += int(pred==gold); n += 1\n",
    "    return ok/max(1,n)\n",
    "\n",
    "# --------- caption\u2013question consistency (for risk\u2013coverage) ---------\n",
    "# Expanded vocab for SLAKE/VQA-style clinical content\n",
    "ORGANS = {\n",
    "    \"head\",\"skull\",\"brain\",\"spine\",\"cervical\",\"thoracic\",\"lumbar\",\"rib\",\"chest\",\"lung\",\n",
    "    \"heart\",\"mediastinum\",\"diaphragm\",\"abdomen\",\"liver\",\"spleen\",\"kidney\",\"pancreas\",\n",
    "    \"pelvis\",\"hip\",\"femur\",\"knee\",\"tibia\",\"fibula\",\"ankle\",\"foot\",\"humerus\",\"elbow\",\n",
    "    \"forearm\",\"wrist\",\"hand\",\"shoulder\",\"clavicle\",\"sinus\"\n",
    "}\n",
    "SIDES = {\"left\",\"right\",\"bilateral\",\"unilateral\"}\n",
    "FINDINGS = {\n",
    "    \"mass\",\"nodule\",\"lesion\",\"fracture\",\"dislocation\",\"opacity\",\"effusion\",\"consolidation\",\n",
    "    \"atelectasis\",\"pneumothorax\",\"pneumonia\",\"edema\",\"calcification\",\"hemorrhage\",\n",
    "    \"enlargement\",\"dilation\",\"hernia\",\"stone\",\"obstruction\",\"metal\",\"catheter\",\"tube\",\"line\"\n",
    "}\n",
    "\n",
    "def norm(s):\n",
    "    # lower, keep spaces/numbers/letters only (handles SLAKE en text; for zh text, semantic sim will dominate)\n",
    "    return re.sub(r\"[^a-z0-9 ]\",\"\", s.lower()) if s else \"\"\n",
    "\n",
    "def consistency_score(q, cap):\n",
    "    if not cap:\n",
    "        return 0.0\n",
    "    qn, cn = norm(q), norm(cap)\n",
    "    # vocabulary coverage\n",
    "    vocab = ORGANS | SIDES | FINDINGS\n",
    "    q_terms = [t for t in vocab if t in qn]\n",
    "    term_cov = sum(1 for t in q_terms if t in cn)/max(1,len(q_terms))\n",
    "    # semantic string similarity (robust to wording)\n",
    "    sem = partial_ratio(qn, cn)/100.0\n",
    "    # weight vocab match higher; fall back to sem if no terms found\n",
    "    alpha = 0.7 if q_terms else 0.3\n",
    "    return alpha*term_cov + (1-alpha)*sem\n",
    "\n",
    "def risk_coverage(records, tau):\n",
    "    kept=[]\n",
    "    for r in records:\n",
    "        if not has_json(r):\n",
    "            continue\n",
    "        q = _get(r, \"question\", \"Question\", default=\"\")\n",
    "        cap = _get(r[\"json\"], \"caption\", \"Caption\", default=\"\")\n",
    "        score = consistency_score(q, cap)\n",
    "        if score >= tau:\n",
    "            kept.append(r)\n",
    "    cov = len(kept)/max(1,len(records))\n",
    "    acc = accuracy(kept) if kept else 0.0\n",
    "    return cov, acc, 1.0-acc"
   ],
   "metadata": {
    "id": "sn9Rt4aiz2uo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Make sure you've already created these with run_protocol before running this cell:\n",
    "# slake_think.jsonl, slake_cap.jsonl in OUT_DIR\n",
    "\n",
    "think200 = [json.loads(x) for x in open(f\"{OUT_DIR}/slake_think.jsonl\",'r',encoding='utf-8')]\n",
    "cap200   = [json.loads(x) for x in open(f\"{OUT_DIR}/slake_cap.jsonl\",'r',encoding='utf-8')]\n",
    "\n",
    "print(\"Parsed:\", sum(has_json(r) for r in cap200), \"/\", len(cap200))\n",
    "print(\"Has boxes:\", sum(has_boxes(r) for r in cap200), \"/\", len(cap200))\n",
    "print(\"Acc Think\u2192Answer (200):\", accuracy(think200))\n",
    "print(\"Acc Caption\u2192Reason\u2192Answer (200):\", accuracy(cap200))\n",
    "\n",
    "for t in [0.3,0.4,0.5,0.6]:\n",
    "    cov, acc, risk = risk_coverage(cap200, t)\n",
    "    print(f\"Risk\u2013Coverage \u03c4={t:.1f}: coverage={cov:.2f}, acc={acc:.2f}, risk={risk:.2f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9NGs-Lez_Ro",
    "outputId": "46f7c671-5a02-4558-bb1d-d61387a86b58"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parsed: 183 / 200\n",
      "Has boxes: 151 / 200\n",
      "Acc Think\u2192Answer (200): 0.385\n",
      "Acc Caption\u2192Reason\u2192Answer (200): 0.35911602209944754\n",
      "Risk\u2013Coverage \u03c4=0.3: coverage=0.39, acc=0.29, risk=0.71\n",
      "Risk\u2013Coverage \u03c4=0.4: coverage=0.39, acc=0.29, risk=0.71\n",
      "Risk\u2013Coverage \u03c4=0.5: coverage=0.39, acc=0.29, risk=0.71\n",
      "Risk\u2013Coverage \u03c4=0.6: coverage=0.36, acc=0.27, risk=0.73\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Counterfactuals on 200 (ROI-CF with shrink + inpaint) ---\n",
    "\n",
    "import json, re, random\n",
    "import numpy as np, cv2\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(0); np.random.seed(0)\n",
    "\n",
    "# assumes: normalize_answer, parse_json_safe, call_vlm, prompt_caption_reason_answer already defined\n",
    "\n",
    "def answer_from_json(js):\n",
    "    return normalize_answer((js or {}).get(\"answer\",\"\"))\n",
    "\n",
    "def boxes_from_json(js):\n",
    "    bx = (js or {}).get(\"boxes\", [])\n",
    "    return bx if isinstance(bx, list) else []\n",
    "\n",
    "def box_area_frac(boxes, w, h):\n",
    "    if not boxes: return 0.0\n",
    "    x1,y1,x2,y2 = [int(v) for v in boxes[0]]\n",
    "    x1 = max(0,min(w-1,x1)); x2 = max(0,min(w-1,x2))\n",
    "    y1 = max(0,min(h-1,y1)); y2 = max(0,min(h-1,y2))\n",
    "    A = max(0, x2-x1) * max(0, y2-y1)\n",
    "    return A / max(1, w*h)\n",
    "\n",
    "def shrink_box(b, w, h, frac=0.2):\n",
    "    x1,y1,x2,y2 = [int(v) for v in b]\n",
    "    cx, cy = (x1+x2)//2, (y1+y2)//2\n",
    "    bw, bh = max(1, x2-x1), max(1, y2-y1)\n",
    "    nx = max(1, int(bw*(1-frac)))\n",
    "    ny = max(1, int(bh*(1-frac)))\n",
    "    x1n, x2n = cx - nx//2, cx + nx//2\n",
    "    y1n, y2n = cy - ny//2, cy + ny//2\n",
    "    x1n = max(0, min(w-1, x1n)); x2n = max(0, min(w-1, x2n))\n",
    "    y1n = max(0, min(h-1, y1n)); y2n = max(0, min(h-1, y2n))\n",
    "    if x2n <= x1n or y2n <= y1n:\n",
    "        return [x1, y1, x2, y2]\n",
    "    return [x1n, y1n, x2n, y2n]\n",
    "\n",
    "def shrink_boxes(boxes, w, h, frac=0.2):\n",
    "    return [shrink_box(boxes[0], w, h, frac)] if boxes else []\n",
    "\n",
    "def inpaint_boxes(image_pil, boxes):\n",
    "    img = np.array(image_pil.convert(\"RGB\"))\n",
    "    h, w = img.shape[:2]\n",
    "    mask = np.zeros((h,w), np.uint8)\n",
    "    for b in boxes:\n",
    "        x1,y1,x2,y2 = [int(v) for v in b]\n",
    "        x1 = max(0, min(w-1, x1)); x2 = max(0, min(w-1, x2))\n",
    "        y1 = max(0, min(h-1, y1)); y2 = max(0, min(h-1, y2))\n",
    "        if x2>x1 and y2>y1:\n",
    "            mask[y1:y2, x1:x2] = 255\n",
    "    if mask.max()==0:\n",
    "        return image_pil\n",
    "    out = cv2.inpaint(img, mask, inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "    return Image.fromarray(out)\n",
    "\n",
    "# load the 200-sample predictions and map dataset items (SLAKE)\n",
    "cap200   = [json.loads(x) for x in open(f\"{OUT_DIR}/slake_cap.jsonl\",'r',encoding='utf-8')]\n",
    "items200 = {i: ds_test[i] for i in range(200)}\n",
    "\n",
    "cf_records = []\n",
    "with_box_total = 0\n",
    "\n",
    "for r in cap200:\n",
    "    js = r.get(\"json\")\n",
    "    if not isinstance(js, dict):\n",
    "        continue\n",
    "    boxes = boxes_from_json(js)\n",
    "    if not boxes:\n",
    "        continue\n",
    "    with_box_total += 1\n",
    "\n",
    "    # r[\"idx\"] should point to ds_test; fallback to position if absent\n",
    "    idx = r.get(\"idx\", None)\n",
    "    if idx is None:\n",
    "        continue\n",
    "\n",
    "    item = items200.get(idx)\n",
    "    if item is None:\n",
    "        continue\n",
    "\n",
    "    img, q = item[\"image\"], item[\"question\"]\n",
    "    w, h = img.size\n",
    "\n",
    "    # skip non-diagnostic huge ROIs\n",
    "    if box_area_frac(boxes, w, h) > 0.70:\n",
    "        continue\n",
    "\n",
    "    # shrink + inpaint (no extra blur)\n",
    "    boxes_s = shrink_boxes(boxes, w, h, frac=0.2)  # try 0.3 if still coarse\n",
    "    img_cf  = inpaint_boxes(img, boxes_s)\n",
    "\n",
    "    raw_cf = call_vlm(img_cf, prompt_caption_reason_answer(q))\n",
    "    js_cf  = parse_json_safe(raw_cf)\n",
    "    ans0   = answer_from_json(js)\n",
    "    ans_cf = answer_from_json(js_cf)\n",
    "    faithful = (bool(ans0) and bool(ans_cf) and (ans_cf != ans0))\n",
    "\n",
    "    r.update({\"json_cf\": js_cf, \"answer_cf\": ans_cf, \"faithful\": faithful})\n",
    "    cf_records.append(r)\n",
    "\n",
    "cf_cov   = len(cf_records) / max(1, with_box_total)\n",
    "flip_roi = sum(1 for r in cf_records if r.get(\"faithful\") is True) / max(1, len(cf_records))\n",
    "\n",
    "print(f\"CF coverage (200, box-cases): {cf_cov:.2f}\")\n",
    "print(f\"ROI-CF flip rate (200): {flip_roi:.2f}\")"
   ],
   "metadata": {
    "id": "VC6c14pb0QH0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Ensure cap200 and items200 exist (SLAKE versions)\n",
    "try:\n",
    "    cap200\n",
    "except NameError:\n",
    "    import json\n",
    "    cap200 = [json.loads(x) for x in open(f\"{OUT_DIR}/slake_cap.jsonl\",'r',encoding='utf-8')]\n",
    "\n",
    "try:\n",
    "    items200\n",
    "except NameError:\n",
    "    # map first 200 test items for quick lookup\n",
    "    items200 = {i: ds_test[i] for i in range(200)}\n",
    "\n",
    "def random_box_like(boxes, w, h):\n",
    "    \"\"\"Sample a random box with SAME size as the (possibly shrunk) first box.\"\"\"\n",
    "    if not boxes:\n",
    "        return []\n",
    "    x1,y1,x2,y2 = [int(v) for v in boxes[0]]\n",
    "    bw, bh = max(1, x2-x1), max(1, y2-y1)\n",
    "    rx1 = np.random.randint(0, max(1, w-bw))\n",
    "    ry1 = np.random.randint(0, max(1, h-bh))\n",
    "    return [[rx1, ry1, rx1+bw, ry1+bh]]\n",
    "\n",
    "rand_flips, rand_total = 0, 0\n",
    "for r in cap200:  # make sure cap200 is loaded\n",
    "    js = r.get(\"json\")\n",
    "    if not isinstance(js, dict) or not boxes_from_json(js):\n",
    "        continue\n",
    "\n",
    "    # use items200 for the first 200 items\n",
    "    idx = r.get(\"idx\", None)\n",
    "    if idx is None:\n",
    "        continue\n",
    "    item = items200.get(idx)\n",
    "    if item is None:\n",
    "        continue\n",
    "\n",
    "    img, q = item[\"image\"], item[\"question\"]\n",
    "    w, h = img.size\n",
    "    boxes = boxes_from_json(js)\n",
    "\n",
    "    # same skip rule as ROI-CF\n",
    "    if box_area_frac(boxes, w, h) > 0.70:\n",
    "        continue\n",
    "\n",
    "    # use SAME shrink as ROI-CF to define a fair size\n",
    "    boxes_s = shrink_boxes(boxes, w, h, frac=0.2)\n",
    "    rboxes  = random_box_like(boxes_s, w, h)\n",
    "\n",
    "    # use inpainting (same intervention as ROI-CF)\n",
    "    img_cf = inpaint_boxes(img, rboxes)\n",
    "    raw_cf = call_vlm(img_cf, prompt_caption_reason_answer(q))\n",
    "    js_cf  = parse_json_safe(raw_cf)\n",
    "\n",
    "    ans0   = answer_from_json(js)\n",
    "    ans_cf = answer_from_json(js_cf)\n",
    "\n",
    "    if bool(ans0) and bool(ans_cf):\n",
    "        rand_flips += int(ans_cf != ans0)\n",
    "        rand_total += 1\n",
    "\n",
    "rand_flip_rate = rand_flips / max(1, rand_total)\n",
    "print(\"Random-CF flip rate (200):\", f\"{rand_flip_rate:.2f}\")"
   ],
   "metadata": {
    "id": "94_KcTzA0sCc"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}